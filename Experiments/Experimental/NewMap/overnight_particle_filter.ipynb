{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Code\n"
     ]
    }
   ],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import json\n",
    "import statistics\n",
    "from typing import Iterator\n",
    "from buslane.commands import CommandHandler\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "from image_provider import ImageProvider\n",
    "from test_runner import TestRunner, RepeatResult, DroneResult, TestRun\n",
    "from map_provider import MapProvider, ImageProjection\n",
    "from DroneProvider import SimulatedDroneProvider\n",
    "from vector import Vector2D\n",
    "from map_plotter import MapPlotter\n",
    "from WeightCalculators.Transformers import StretchedTanTransformer\n",
    "from WeightCalculators import ModelBasedWeightCalculator\n",
    "from know_location_drone_localizer import KnownLocationDroneLocalizer, StepCommand\n",
    "\n",
    "from ModelBuilders import TestEfficientNetB0Builder, TestMobileNetBuilder, TestVgg16NetBuilder, TestEfficientNetB2Builder, TestResNetBuilder, TestBaseModelBuilder, ModelOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_gpu_memory_growth():\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    \"\"\"\n",
    "    Enables memory growth mode for GPUs.\n",
    "    \"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    assert len(gpus) > 0, \"No GPUs detected!\"\n",
    "            \n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 672\n",
    "accuracy_threshold = 25\n",
    "particle_counts = [1500, 1000, 500, 250, 150, 100, 75]\n",
    "particle_randomize_percent = 0\n",
    "\n",
    "drone_move_noise = 30\n",
    "drone_last_known_noise = 50\n",
    "initial_location_offset = 25\n",
    "\n",
    "repeat_count = 10\n",
    "drone_steps = 25\n",
    "graph_size = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_at_45d(distance: int) -> int:\n",
    "    coord = math.sqrt(math.pow(distance, 2) / 2)\n",
    "    return math.floor(coord)\n",
    "\n",
    "def build_drones(crop_size: int) -> Iterator[TestRun]:\n",
    "    #6024x6024\n",
    "    city_image = ImageProvider(\"City/NewTraining/ExperimentZone/City_2017.jpg\")\n",
    "    drone_image = ImageProvider(\"City/NewTraining/ExperimentZone/City_2016.jpg\")\n",
    "\n",
    "    projection = ImageProjection(Vector2D(0, 524), Vector2D(5000, 5000))\n",
    "    city_map = MapProvider(\n",
    "        image_provider=city_image,\n",
    "        crop_size=crop_size,\n",
    "        projection=projection)\n",
    "    \n",
    "    drone_map = MapProvider(\n",
    "        image_provider=drone_image,\n",
    "        crop_size=crop_size,\n",
    "        projection=projection)\n",
    "    \n",
    "    run_name = \"Forest\"\n",
    "    for move_by in [60, 100]:\n",
    "        yield TestRun(run_name, SimulatedDroneProvider(drone_map, Vector2D(1000, 2500), Vector2D(move_by, 0), drone_steps), city_map)\n",
    "\n",
    "    projection = ImageProjection(Vector2D(524, 0), Vector2D(5000, 5000))\n",
    "    city_map = MapProvider(\n",
    "        image_provider=city_image,\n",
    "        crop_size=crop_size,\n",
    "        projection=projection)\n",
    "    \n",
    "    drone_map = MapProvider(\n",
    "        image_provider=drone_image,\n",
    "        crop_size=crop_size,\n",
    "        projection=projection)\n",
    "    \n",
    "    run_name = \"Residential\"\n",
    "    for move_by in [60, 100]:\n",
    "        move_by_side = get_distance_at_45d(move_by)\n",
    "        yield TestRun(run_name, SimulatedDroneProvider(drone_map, Vector2D(2500, 1000), Vector2D(move_by_side, move_by_side), drone_steps), city_map)\n",
    "\n",
    "    projection = ImageProjection(Vector2D(0, 1000), Vector2D(5000, 5000))\n",
    "    city_map = MapProvider(\n",
    "        image_provider=city_image,\n",
    "        crop_size=crop_size,\n",
    "        projection=projection)\n",
    "    \n",
    "    drone_map = MapProvider(\n",
    "        image_provider=drone_image,\n",
    "        crop_size=crop_size,\n",
    "        projection=projection)\n",
    "    \n",
    "    run_name = \"Apartments\"\n",
    "    for move_by in [60, 100]:\n",
    "        yield TestRun(run_name, SimulatedDroneProvider(drone_map, Vector2D(2500, 4000), Vector2D(0, -move_by), drone_steps), city_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_drone_folder(base_folder: str, run_name: str, drone_repr: str):\n",
    "    drone_dir = f\"{base_folder}/{run_name}/drone-{drone_repr}\"\n",
    "    os.makedirs(drone_dir, exist_ok=True)\n",
    "\n",
    "    return drone_dir\n",
    "\n",
    "def _build_repeat_folder(base_folder: str, run_name: str, drone_repr: str, repeat_index: int):\n",
    "    drone_folder = _build_drone_folder(base_folder, run_name, drone_repr)\n",
    "\n",
    "    repeat_dir = f\"{drone_folder}/repeat-{repeat_index:02d}\"\n",
    "    os.makedirs(repeat_dir, exist_ok=True)\n",
    "\n",
    "    return repeat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepCommandHandler(CommandHandler[StepCommand]):\n",
    "    def __init__(self, base_folder: str, run_name: str, plotter: MapPlotter):\n",
    "        self.base_folder = base_folder\n",
    "        self.run_name = run_name\n",
    "\n",
    "        self.plotter = plotter\n",
    "\n",
    "    def handle(self, command: StepCommand) -> None:\n",
    "        drone = command.drone\n",
    "\n",
    "        # fig = self.plotter.plot_graph(\n",
    "        #     particles=command.particles,\n",
    "        #     weights=command.weights,\n",
    "        #     title=\"Drono lokalizavimas\",\n",
    "        #     drone_x=drone.get_position().x,\n",
    "        #     drone_y=drone.get_position().y,\n",
    "        #     predicted_x=command.probable_position.x,\n",
    "        #     predicted_y=command.probable_position.y)\n",
    "\n",
    "        # fig.show()\n",
    "\n",
    "        prediction_distance = drone.get_position().distance_to(command.probable_position)\n",
    "        print(f\"\\t\\t Flying... Step {drone.get_current_step()}. Distance: {prediction_distance}\")\n",
    "\n",
    "        # folder = _build_repeat_folder(self.base_folder, self.run_name, f\"{drone}\", drone.get_reset_count())\n",
    "        # filename = f\"{folder}/step_{drone.get_current_step():02d}.jpg\"\n",
    "\n",
    "        # fig.write_image(filename, engine=\"kaleido\")\n",
    "\n",
    "class RepeatResultHandler(CommandHandler[RepeatResult]):\n",
    "    def __init__(self, base_folder: str, run_name: str,):\n",
    "        self.base_folder = base_folder\n",
    "        self.run_name = run_name\n",
    "\n",
    "    def handle(self, command: RepeatResult) -> None:\n",
    "        repeat_folder = _build_repeat_folder(self.base_folder, self.run_name, command.drone_repr, command.repeat_index)\n",
    "        repeat_file = f\"{repeat_folder}/repeat-stats.json\"\n",
    "\n",
    "        with open(repeat_file, \"w\") as file:\n",
    "            file.write(json.dumps(command.dump(), indent=4, sort_keys=True))\n",
    "\n",
    "class DroneResultHandler(CommandHandler[DroneResult]):\n",
    "    def __init__(self, base_folder: str, run_name: str,):\n",
    "        self.base_folder = base_folder\n",
    "        self.run_name = run_name\n",
    "\n",
    "    def handle(self, command: DroneResult) -> None:\n",
    "        drone_folder = _build_drone_folder(self.base_folder, self.run_name, command.drone_repr)\n",
    "        drone_file = f\"{drone_folder}/route-stats.json\"\n",
    "\n",
    "        with open(drone_file, \"w\") as file:\n",
    "            file.write(json.dumps(command.dump(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleFilterRunner:\n",
    "    def __init__(self, model_builder: TestBaseModelBuilder, similarity_threshold: float, particle_count: int):\n",
    "        self.model_builder = model_builder\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.particle_count = particle_count\n",
    "\n",
    "        base_folder = f\"Data/ParticleFilter/BestOf/{crop_size}/Particles-{particle_count}\"\n",
    "\n",
    "        builder_options = model_builder.get_options()\n",
    "        builder_label = builder_options.builder_label\n",
    "        representation = builder_options.representation()\n",
    "\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d.%H-%M-%S\")\n",
    "        self.run_folder = f\"{base_folder}/{builder_label}/{representation}/{current_date}\"\n",
    "\n",
    "    def _get_distances(self, results: list[DroneResult]):\n",
    "        distances = []\n",
    "        for result in results:\n",
    "            for repeat in result.repeat_results:\n",
    "                for step in repeat.step_results:\n",
    "                    distances.append(step.distance)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def save_all_runs(self, results: list[DroneResult]):\n",
    "        run_data = {\n",
    "            \"average_acc\": statistics.mean([result.average_accuracy for result in results]),\n",
    "            \"average_distance\": statistics.mean(self._get_distances(results)),\n",
    "            \"distance_std\": statistics.stdev(self._get_distances(results)),\n",
    "            \"max_distance\": max(self._get_distances(results)),\n",
    "            \"min_distance\": min(self._get_distances(results)),\n",
    "            \"runs\": [\n",
    "                {\n",
    "                    \"drone\": result.drone_repr,\n",
    "                    \"average_accuracy\": result.average_accuracy, \n",
    "                    \"average_distance\": result.avg_distances(),\n",
    "                    \"distance_std\": result.distances_std(),\n",
    "                    \"min_distance\": min(result.get_distances()),\n",
    "                    \"max_distance\": max(result.get_distances()),\n",
    "                }\n",
    "                for result in results\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        run_file = f\"{self.run_folder}/run-stats.json\"\n",
    "        with open(run_file, \"w\") as file:\n",
    "            file.write(json.dumps(run_data, indent=4))\n",
    "\n",
    "    def run(self):\n",
    "        model_calculator = ModelBasedWeightCalculator(\n",
    "            self.model_builder,\n",
    "            batch_size=48,\n",
    "            transformer=StretchedTanTransformer(self.similarity_threshold))\n",
    "\n",
    "        localizer = KnownLocationDroneLocalizer(\n",
    "            particle_count=self.particle_count,\n",
    "            weight_calculator=model_calculator,\n",
    "            particle_randomize_percent=particle_randomize_percent,\n",
    "            drone_move_noise=drone_move_noise,\n",
    "            drone_last_known_noise=drone_last_known_noise,\n",
    "            initial_location_offset=initial_location_offset)\n",
    "\n",
    "        runner = TestRunner(localizer, accuracy_threshold, repeat_count=repeat_count)\n",
    "\n",
    "        test_runs = build_drones(crop_size=crop_size)\n",
    "\n",
    "        results: list[DroneResult] = []\n",
    "        for test_run in test_runs:\n",
    "            name, drone, city_map = test_run.name, test_run.drone, test_run.city_map\n",
    "\n",
    "            plotter = MapPlotter(city_map, graph_size=graph_size)\n",
    "\n",
    "            runner.command_bus.clear()\n",
    "            runner.command_bus.register(handler=RepeatResultHandler(self.run_folder, name))\n",
    "\n",
    "            localizer.command_bus.clear()\n",
    "            localizer.command_bus.register(handler=StepCommandHandler(self.run_folder, name, plotter=plotter))\n",
    "\n",
    "            print(f\"Starting Drone Route: {drone}\")\n",
    "            drone_result = runner.run(test_run)\n",
    "            print(f\"Ended Drone Route: {drone}. Average accuracy: {drone_result.average_accuracy}\")\n",
    "\n",
    "            results.append(drone_result)\n",
    "            DroneResultHandler(self.run_folder, name).handle(drone_result)\n",
    "\n",
    "        self.save_all_runs(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelNN:\n",
    "    modelNN: int\n",
    "    trainable_froms: list[int]\n",
    "\n",
    "@dataclass\n",
    "class BuilderTests:\n",
    "    def __init__(self, \n",
    "        ingest_denses: list[int],\n",
    "        output_denses: list[int],\n",
    "        modelNNs: list[ModelNN],\n",
    "        epochs_list: list[int],\n",
    "        batch_list: list[int],\n",
    "        builder_label: str):\n",
    "\n",
    "        self.ingest_denses = ingest_denses\n",
    "        self.output_denses = output_denses\n",
    "        self.modelNNs = modelNNs\n",
    "        self.epochs_list = epochs_list\n",
    "        self.batch_list = batch_list\n",
    "\n",
    "        self.builder_label = builder_label\n",
    "        \n",
    "    def generate_options(self):\n",
    "        tests = product(self.ingest_denses, self.output_denses, self.modelNNs, self.epochs_list, self.batch_list)\n",
    "\n",
    "        for ingest_dense, output_dense, modellNN, epochs, batch_size in tests:\n",
    "            for trainable_from in modellNN.trainable_froms:\n",
    "                if trainable_from > modellNN.modelNN:\n",
    "                    continue\n",
    "\n",
    "                yield ModelOptions(\n",
    "                    builder_label=self.builder_label,\n",
    "                    model_nn=modellNN.modelNN,\n",
    "                    ingest_dense=ingest_dense,\n",
    "                    output_dense=output_dense,\n",
    "                    trainable_from_index=trainable_from,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effb0_similarities = [0.662, 0.763, 0.654, 0.651, 0.722]\n",
    "efficient_b0_tests = BuilderTests(\n",
    "    ingest_denses=[64],\n",
    "    output_denses=[8],\n",
    "    modelNNs=[\n",
    "        ModelNN(30, [0, 30]),\n",
    "        ModelNN(71, [30]),\n",
    "        ModelNN(140, [140]),\n",
    "        ModelNN(254, [254]),\n",
    "    ],\n",
    "    epochs_list=[6],\n",
    "    batch_list=[16],\n",
    "    builder_label=\"Experimental/NewTraining/EfficientNetV2B0\")\n",
    "\n",
    "\n",
    "for test in efficient_b0_tests.generate_options():\n",
    "    print(test.representation())\n",
    "\n",
    "def run_efficient_net(options, similarity, particle_count):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "    enable_gpu_memory_growth()\n",
    "\n",
    "    model_builder = TestEfficientNetB0Builder(options)\n",
    "    particle_filter = ParticleFilterRunner(model_builder, similarity, particle_count)\n",
    "\n",
    "    particle_filter.run()\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for options, similarity in zip(efficient_b0_tests.generate_options(), effb0_similarities):\n",
    "    for particle_count in particle_counts:\n",
    "        p1 = multiprocessing.Process(target=run_efficient_net, args=[options, similarity, particle_count])\n",
    "\n",
    "        p1.start()\n",
    "        p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effb2_similarities = [0.721, 0.84, 0.722, 0.697, 0.944, 0.861]\n",
    "efficient_b2_tests = BuilderTests(\n",
    "    ingest_denses=[64],\n",
    "    output_denses=[8],\n",
    "    modelNNs=[\n",
    "        ModelNN(59, [59, 25]),\n",
    "        ModelNN(68, [25, 0]),\n",
    "        ModelNN(111, [111]),\n",
    "        ModelNN(331, [331]),\n",
    "    ],\n",
    "    epochs_list=[6],\n",
    "    batch_list=[16],\n",
    "    builder_label=\"Experimental/NewTraining/EfficientNetB2\")\n",
    "\n",
    "for test in efficient_b2_tests.generate_options():\n",
    "    print(test.representation())\n",
    "\n",
    "def run_efficientb2_net(options, similarity, particle_count):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    enable_gpu_memory_growth()\n",
    "\n",
    "    model_builder = TestEfficientNetB2Builder(options)\n",
    "    particle_filter = ParticleFilterRunner(model_builder, similarity, particle_count)\n",
    "\n",
    "    particle_filter.run()\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for options, similarity in zip(efficient_b2_tests.generate_options(), effb2_similarities):\n",
    "    for particle_count in particle_counts:\n",
    "        p1 = multiprocessing.Process(target=run_efficientb2_net, args=[options, similarity, particle_count])\n",
    "\n",
    "        p1.start()\n",
    "        p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_similarities = [0.699, 0.704, 0.728]\n",
    "vgg_tests = BuilderTests(\n",
    "    ingest_denses=[64],\n",
    "    output_denses=[8],\n",
    "    modelNNs=[\n",
    "        ModelNN(10, [10]),\n",
    "        ModelNN(14, [14]),\n",
    "        ModelNN(18, [18]),\n",
    "    ],\n",
    "    epochs_list=[6],\n",
    "    batch_list=[4],\n",
    "    builder_label=\"Experimental/NewTraining/VGG16\")\n",
    "\n",
    "for test in vgg_tests.generate_options():\n",
    "    print(test.representation())\n",
    "\n",
    "def run_vgg16_net(options, similarity, particle_count):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    enable_gpu_memory_growth()\n",
    "\n",
    "    model_builder = TestVgg16NetBuilder(options)\n",
    "    particle_filter = ParticleFilterRunner(model_builder, similarity, particle_count)\n",
    "\n",
    "    particle_filter.run()\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for options, similarity in zip(vgg_tests.generate_options(), vgg_similarities):\n",
    "    for particle_count in particle_counts:\n",
    "        p1 = multiprocessing.Process(target=run_vgg16_net, args=[options, similarity, particle_count])\n",
    "\n",
    "        p1.start()\n",
    "        p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_similarities = [0.735, 0.78, 0.719, 0.66, 0.719]\n",
    "resnet_tests = BuilderTests(\n",
    "    ingest_denses=[128],\n",
    "    output_denses=[8],\n",
    "    modelNNs=[\n",
    "        ModelNN(50, [0, 50]),  \n",
    "        ModelNN(80, [0, 38]),\n",
    "        ModelNN(142, [142]),\n",
    "    ],\n",
    "    epochs_list=[6],\n",
    "    batch_list=[16],\n",
    "    builder_label=\"Experimental/NewTraining/ResNet50\")\n",
    "\n",
    "for test in resnet_tests.generate_options():\n",
    "    print(test.representation())\n",
    "\n",
    "def run_resnet_net(options, similarity, particle_count):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "    enable_gpu_memory_growth()\n",
    "\n",
    "    model_builder = TestResNetBuilder(options)\n",
    "    particle_filter = ParticleFilterRunner(model_builder, similarity, particle_count)\n",
    "\n",
    "    particle_filter.run()\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for options, similarity in zip(resnet_tests.generate_options(), resnet_similarities):\n",
    "    for particle_count in particle_counts:\n",
    "        p1 = multiprocessing.Process(target=run_resnet_net, args=[options, similarity, particle_count])\n",
    "\n",
    "        p1.start()\n",
    "        p1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_similarities = [0.691, 0.853, 0.688, 0.692, 0.7, 0.841]\n",
    "mobile_tests = BuilderTests(\n",
    "    ingest_denses=[64],\n",
    "    output_denses=[8],\n",
    "    modelNNs=[\n",
    "        ModelNN(35, [0, 35]),\n",
    "        ModelNN(54, [0, 35]),\n",
    "        ModelNN(72, [0, 72]),\n",
    "    ],\n",
    "    epochs_list=[6],\n",
    "    batch_list=[16],\n",
    "    builder_label=\"Experimental/NewTraining/MobileNet\")\n",
    "\n",
    "for test in mobile_tests.generate_options():\n",
    "    print(test.representation())\n",
    "\n",
    "def run_mobile_net(options, similarity, particle_count):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    enable_gpu_memory_growth()\n",
    "\n",
    "    model_builder = TestMobileNetBuilder(options)\n",
    "    particle_filter = ParticleFilterRunner(model_builder, similarity, particle_count)\n",
    "\n",
    "    particle_filter.run()\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for options, similarity in zip(mobile_tests.generate_options(), mobile_similarities):\n",
    "    for particle_count in particle_counts:\n",
    "        p1 = multiprocessing.Process(target=run_mobile_net, args=[options, similarity, particle_count])\n",
    "\n",
    "        p1.start()\n",
    "        p1.join()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
